#!/bin/bash

# Start Ollama with qwen2.5vl:7b Model
# This script ensures Ollama is running with the correct model for the dev server

set -e

echo "ğŸš€ Starting Ollama with qwen2.5vl:7b model..."

# Configuration
MODEL_NAME="qwen2.5vl:7b"
OLLAMA_PORT=11434

# Check if Ollama is already running
if curl -s http://localhost:$OLLAMA_PORT/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama is already running"
else
    echo "ğŸ”„ Starting Ollama service..."
    ollama serve &
    
    # Wait for Ollama to start
    echo "â³ Waiting for Ollama to start..."
    for i in {1..30}; do
        if curl -s http://localhost:$OLLAMA_PORT/api/tags > /dev/null 2>&1; then
            echo "âœ… Ollama started successfully"
            break
        fi
        sleep 1
    done
    
    if ! curl -s http://localhost:$OLLAMA_PORT/api/tags > /dev/null 2>&1; then
        echo "âŒ Failed to start Ollama"
        exit 1
    fi
fi

# Check if model is available
echo "ğŸ” Checking qwen2.5vl:7b model..."
if ollama list | grep -q "$MODEL_NAME"; then
    echo "âœ… qwen2.5vl:7b model is available"
else
    echo "ğŸ“¥ Downloading qwen2.5vl:7b model..."
    ollama pull "$MODEL_NAME"
fi

# Test model
echo "ğŸ§ª Testing qwen2.5vl:7b model..."
if ollama run "$MODEL_NAME" "Hello" > /dev/null 2>&1; then
    echo "âœ… qwen2.5vl:7b model is working"
else
    echo "âš ï¸ qwen2.5vl:7b model test failed, but continuing..."
fi

echo "ğŸ‰ Ollama is ready with qwen2.5vl:7b model!"
echo "ğŸ“ Model: $MODEL_NAME"
echo "ğŸŒ Server: http://localhost:$OLLAMA_PORT" 